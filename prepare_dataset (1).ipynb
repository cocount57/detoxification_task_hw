{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1722de0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4014cd7c",
   "metadata": {},
   "source": [
    "## EVALUATOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9934fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "from functools import partial\n",
    "from typing import Optional, Type, Tuple, Dict, Callable, List, Union\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sacrebleu import CHRF\n",
    "from tqdm.auto import trange\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49361820",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_target_label(\n",
    "    model: AutoModelForSequenceClassification, target_label: Union[int, str]\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Prepare the target label to ensure it is valid for the given model.\n",
    "\n",
    "    Args:\n",
    "        model (AutoModelForSequenceClassification): Text classification model.\n",
    "        target_label (Union[int, str]): The target label to prepare.\n",
    "\n",
    "    Returns:\n",
    "        int: The prepared target label.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the target_label is not found in model labels or ids.\n",
    "    \"\"\"\n",
    "    if target_label in model.config.id2label:\n",
    "        pass\n",
    "    elif target_label in model.config.label2id:\n",
    "        target_label = model.config.label2id.get(target_label)\n",
    "    elif (\n",
    "        isinstance(target_label, str)\n",
    "        and target_label.isnumeric()\n",
    "        and int(target_label) in model.config.id2label\n",
    "    ):\n",
    "        target_label = int(target_label)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f'target_label \"{target_label}\" not in model labels or ids: {model.config.id2label}.'\n",
    "        )\n",
    "    assert isinstance(target_label, int)\n",
    "    return target_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf25d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_texts(\n",
    "    model: AutoModelForSequenceClassification,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    texts: List[str],\n",
    "    target_label: Union[int, str],\n",
    "    second_texts: Optional[List[str]] = None,\n",
    "    batch_size: int = 32,\n",
    "    raw_logits: bool = False,\n",
    "    desc: Optional[str] = \"Calculating STA scores\",\n",
    ") -> npt.NDArray[np.float64]:\n",
    "    \"\"\"\n",
    "    Classify a list of texts using the given model and tokenizer.\n",
    "\n",
    "    Args:\n",
    "        model (AutoModelForSequenceClassification): Text classification model.\n",
    "        tokenizer (AutoTokenizer): The tokenizer corresponding to the model.\n",
    "        texts (List[str]): List of texts to classify.\n",
    "        target_label (Union[int, str]): The target label for classification.\n",
    "        second_texts (Optional[List[str]]): List of secondary texts (not needed by default).\n",
    "        batch_size (int): Batch size for inference.\n",
    "        raw_logits (bool): Whether to return raw logits instead of probs.\n",
    "        desc (Optional[str]): Description for tqdm progress bar.\n",
    "\n",
    "    Returns:\n",
    "        npt.NDArray[np.float64]: Array of classification scores for the texts.\n",
    "    \"\"\"\n",
    "\n",
    "    target_label = prepare_target_label(model, target_label)\n",
    "\n",
    "    res = []\n",
    "\n",
    "    for i in trange(0, len(texts), batch_size, desc=desc):\n",
    "        inputs = [texts[i : i + batch_size]]\n",
    "\n",
    "        if second_texts is not None:\n",
    "            inputs.append(second_texts[i : i + batch_size])\n",
    "        inputs = tokenizer(\n",
    "            *inputs,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "        ).to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                logits = model(**inputs).logits\n",
    "                if raw_logits:\n",
    "                    preds = logits[:, target_label]\n",
    "                elif logits.shape[-1] > 1:\n",
    "                    preds = torch.softmax(logits, -1)[:, target_label]\n",
    "                else:\n",
    "                    preds = torch.sigmoid(logits)[:, 0]\n",
    "                preds = preds.cpu().numpy()\n",
    "            except:\n",
    "                print(i, i + batch_size)\n",
    "                preds = [0] * len(inputs)\n",
    "        res.append(preds)\n",
    "    return np.concatenate(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90505c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_sta(\n",
    "    model: AutoModelForSequenceClassification,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    texts: List[str],\n",
    "    target_label: int = 1,  # 1 is polite, 0 is toxic\n",
    "    batch_size: int = 32,\n",
    ") -> npt.NDArray[np.float64]:\n",
    "    \"\"\"\n",
    "    Evaluate the STA of a list of texts using the given model and tokenizer.\n",
    "\n",
    "    Args:\n",
    "        model (AutoModelForSequenceClassification): Text classification model.\n",
    "        tokenizer (AutoTokenizer): The tokenizer corresponding to the model.\n",
    "        texts (List[str]): List of texts to evaluate.\n",
    "        target_label (int): The target label for style evaluation.\n",
    "        batch_size (int): Batch size for inference.\n",
    "\n",
    "    Returns:\n",
    "        npt.NDArray[np.float64]: Array of STA scores for the texts.\n",
    "    \"\"\"\n",
    "    target_label = prepare_target_label(model, target_label)\n",
    "    scores = classify_texts(\n",
    "        model, tokenizer, texts, target_label, batch_size=batch_size, desc=\"Style\"\n",
    "    )\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7867c38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_sim(\n",
    "    model: SentenceTransformer,\n",
    "    original_texts: List[str],\n",
    "    rewritten_texts: List[str],\n",
    "    batch_size: int = 32,\n",
    "    efficient_version: bool = False,\n",
    ") -> npt.NDArray[np.float64]:\n",
    "    \"\"\"\n",
    "    Evaluate the semantic similarity between original and rewritten texts.\n",
    "    Note that the subtraction is done due to the implementation of the `cosine` metric in `scipy`.\n",
    "    For more details see: https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cosine.html\n",
    "\n",
    "    Args:\n",
    "        model (SentenceTransformer): The sentence transformer model.\n",
    "        original_texts (List[str]): List of original texts.\n",
    "        rewritten_texts (List[str]): List of rewritten texts.\n",
    "        batch_size (int): Batch size for inference.\n",
    "        efficient_version (bool): To use efficient calculation method.\n",
    "\n",
    "    Returns:\n",
    "        npt.NDArray[np.float64]: Array of semantic similarity scores between \\\n",
    "              original and rewritten texts.\n",
    "    \"\"\"\n",
    "    similarities = []\n",
    "\n",
    "    batch_size = min(batch_size, len(original_texts))\n",
    "    for i in trange(0, len(original_texts), batch_size, desc=\"Calculating SIM scores\"):\n",
    "        original_batch = original_texts[i : i + batch_size]\n",
    "        rewritten_batch = rewritten_texts[i : i + batch_size]\n",
    "\n",
    "        embeddings = model.encode(original_batch + rewritten_batch)\n",
    "        original_embeddings = embeddings[: len(original_batch)]\n",
    "        rewritten_embeddings = embeddings[len(original_batch) :]\n",
    "\n",
    "        if efficient_version:\n",
    "            similarity_matrix = np.dot(original_embeddings, rewritten_embeddings.T)\n",
    "            original_norms = np.linalg.norm(original_embeddings, axis=1)\n",
    "            rewritten_norms = np.linalg.norm(rewritten_embeddings, axis=1)\n",
    "            similarities.extend(\n",
    "                1\n",
    "                - similarity_matrix / (np.outer(original_norms, rewritten_norms) + 1e-9)\n",
    "            )\n",
    "        else:\n",
    "            t = [\n",
    "                1 - cosine(original_embedding, rewritten_embedding)\n",
    "                for original_embedding, rewritten_embedding in zip(\n",
    "                    original_embeddings, rewritten_embeddings\n",
    "                )\n",
    "            ]\n",
    "            similarities.extend(t)\n",
    "    return similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dc3cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_style_transfer(\n",
    "    original_texts: List[str],\n",
    "    rewritten_texts: List[str],\n",
    "    style_model: AutoModelForSequenceClassification,\n",
    "    style_tokenizer: AutoTokenizer,\n",
    "    meaning_model: AutoModelForSequenceClassification,\n",
    "    references: Optional[List[str]] = None,\n",
    "    style_target_label: int = 1,\n",
    "    batch_size: int = 32,\n",
    ") -> Dict[str, npt.NDArray[np.float64]]:\n",
    "    \"\"\"\n",
    "    Wrapper for calculating sub-metrics and joint metric.\n",
    "\n",
    "    Args:\n",
    "        original_texts (List[str]): List of original texts.\n",
    "        rewritten_texts (List[str]): List of rewritten texts.\n",
    "        style_model (AutoModelForSequenceClassification): The style classification model.\n",
    "        style_tokenizer (AutoTokenizer): The tokenizer corresponding to the style model.\n",
    "        meaning_model (AutoModelForSequenceClassification): The meaning classification model.\n",
    "        references (Optional[List[str]]): List of reference texts (if available).\n",
    "        style_target_label (int): The target label for style classification.\n",
    "        batch_size (int): Batch size for inference.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, npt.NDArray[np.float64]]: Dictionary containing evaluation metrics.\n",
    "    \"\"\"\n",
    "    accuracy = evaluate_sta(\n",
    "        style_model,\n",
    "        style_tokenizer,\n",
    "        rewritten_texts,\n",
    "        target_label=style_target_label,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "    similarity = evaluate_sim(\n",
    "        model=meaning_model,\n",
    "        original_texts=original_texts,\n",
    "        rewritten_texts=rewritten_texts,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "    result = {\n",
    "        \"STA\": accuracy,\n",
    "        \"SIM\": similarity,\n",
    "    }\n",
    "\n",
    "    if references is not None:\n",
    "\n",
    "        chrf = CHRF()\n",
    "\n",
    "        result[\"CHRF\"] = np.array(\n",
    "            [\n",
    "                chrf.sentence_score(hypothesis=rewritten, references=[reference]).score\n",
    "                / 100\n",
    "                for rewritten, reference in zip(rewritten_texts, references)\n",
    "            ],\n",
    "            dtype=np.float64,\n",
    "        )\n",
    "\n",
    "        result[\"J\"] = result[\"STA\"] * result[\"SIM\"] * result[\"CHRF\"]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a041d9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(\n",
    "    model_name: Optional[str] = None,\n",
    "    model: Optional[AutoModelForSequenceClassification] = None,\n",
    "    tokenizer: Optional[AutoTokenizer] = None,\n",
    "    model_class: Type[\n",
    "        AutoModelForSequenceClassification\n",
    "    ] = AutoModelForSequenceClassification,\n",
    "    use_cuda: bool = True,\n",
    ") -> Tuple[AutoModelForSequenceClassification, AutoTokenizer]:\n",
    "    \"\"\"\n",
    "    Load a pre-trained model and tokenizer from Hugging Face Hub.\n",
    "\n",
    "    Args:\n",
    "        model_name (Optional[str]): The name of the model to load.\n",
    "        model (Optional[AutoModelForSequenceClassification]): A pre-loaded model instance.\n",
    "        tokenizer (Optional[AutoTokenizer]): A pre-loaded tokenizer instance.\n",
    "        model_class (Type[AutoModelForSequenceClassification]): The class of the model to load.\n",
    "        use_cuda (bool): Whether to use CUDA for GPU acceleration.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[AutoModelForSequenceClassification, AutoTokenizer]: The loaded model and tokenizer.\n",
    "    \"\"\"\n",
    "    if model_name == \"sentence-transformers/LaBSE\":\n",
    "        model = SentenceTransformer(\"sentence-transformers/LaBSE\")\n",
    "        return model\n",
    "    if model is None:\n",
    "        if model_name is None:\n",
    "            raise ValueError(\"Either model or model_name should be provided\")\n",
    "        model = model_class.from_pretrained(model_name)\n",
    "\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            model.cuda()\n",
    "    if tokenizer is None:\n",
    "        if model_name is None:\n",
    "            raise ValueError(\"Either tokenizer or model_name should be provided\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe336a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prototext(measure: str, value: str) -> str:\n",
    "    \"\"\"\n",
    "    Format evaluation metrics into prototext format.\n",
    "\n",
    "    Args:\n",
    "        measure (str): The name of the evaluation measure.\n",
    "        value (str): The value of the evaluation measure.\n",
    "\n",
    "    Returns:\n",
    "        str: The formatted prototext string.\n",
    "    \"\"\"\n",
    "    return f'measure{{\\n  key: \"{measure}\"\\n  value: \"{value}\"\\n}}\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda5a095",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation(\n",
    "    input: str,\n",
    "    prediction: str,\n",
    "    output: str,\n",
    "    evaluator: Callable[..., Dict[str, npt.NDArray[np.float64]]],\n",
    ") -> Dict[str, npt.NDArray[np.float64]]:\n",
    "    \"\"\"\n",
    "    Run evaluation on input data using the specified evaluator.\n",
    "\n",
    "    Args:\n",
    "        args (argparse.Namespace): Parsed command-line arguments.\n",
    "        evaluator (Callable[..., Dict[str, npt.NDArray[np.float64]]]): The evaluation function.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, npt.NDArray[np.float64]]: Dictionary containing evaluation results.\n",
    "    \"\"\"\n",
    "    df_input = pd.read_json(input, convert_dates=False, lines=True)\n",
    "    df_input = df_input[[\"id\", \"text\"]]\n",
    "    df_input.set_index(\"id\", inplace=True)\n",
    "    df_input.rename(columns={\"text\": \"input\"}, inplace=True)\n",
    "\n",
    "    df_prediction = pd.read_json(prediction, convert_dates=False, lines=True)\n",
    "    df_prediction = df_prediction[[\"id\", \"text\"]]\n",
    "    df_prediction.set_index(\"id\", inplace=True)\n",
    "    df_prediction.rename(columns={\"text\": \"prediction\"}, inplace=True)\n",
    "\n",
    "    df = df_input.join(df_prediction)\n",
    "\n",
    "    \n",
    "    assert (\n",
    "        len(df) == len(df_input) == len(df_prediction)\n",
    "    ), f\"Dataset lengths {len(df_input)} & {len(df_prediction)} != {len(df)}\"\n",
    "\n",
    "    assert not df.isna().values.any(), \"Datasets contain missing entries\"\n",
    "\n",
    "    result = evaluator(\n",
    "        original_texts=df[\"input\"].tolist(),\n",
    "        rewritten_texts=df[\"prediction\"].tolist(),\n",
    "        references=None,\n",
    "    )\n",
    "\n",
    "    aggregated = {measure: np.mean(values).item() for measure, values in result.items()}\n",
    "\n",
    "    for measure, value in aggregated.items():\n",
    "        output.write(format_prototext(measure, str(value)))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb0034a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main() -> None:\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"-i\",\n",
    "        \"--input\",\n",
    "        type=argparse.FileType(\"rb\"),\n",
    "        required=True,\n",
    "        help=\"Initial texts before style transfer\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-g\",\n",
    "        \"--golden\",\n",
    "        type=argparse.FileType(\"rb\"),\n",
    "        required=False,\n",
    "        help=\"Ground truth texts after style transfer\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-o\",\n",
    "        \"--output\",\n",
    "        type=argparse.FileType(\"w\", encoding=\"UTF-8\"),\n",
    "        default=sys.stdout,\n",
    "        help=\"Path where to write the evaluation results\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--no-cuda\", action=\"store_true\", default=False, help=\"Disable use of CUDA\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--prediction\", type=argparse.FileType(\"rb\"), help=\"Your model predictions\"\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    style_model, style_tokenizer = load_model(\n",
    "        \"textdetox/xlmr-large-toxicity-classifier\", use_cuda=not args.no_cuda\n",
    "    )\n",
    "    meaning_model = load_model(\"sentence-transformers/LaBSE\", use_cuda=not args.no_cuda)\n",
    "\n",
    "    run_evaluation(\n",
    "        args,\n",
    "        evaluator=partial(\n",
    "            evaluate_style_transfer,\n",
    "            style_model=style_model,\n",
    "            style_tokenizer=style_tokenizer,\n",
    "            meaning_model=meaning_model,\n",
    "            style_target_label=0,\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc27657",
   "metadata": {},
   "outputs": [],
   "source": [
    "style_model, style_tokenizer = load_model(\n",
    "        \"textdetox/xlmr-large-toxicity-classifier\"\n",
    "    )\n",
    "meaning_model = load_model(\"sentence-transformers/LaBSE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c8c7ed",
   "metadata": {},
   "source": [
    "## LOAD DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41a21cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"textdetox/multilingual_paradetox\", cache_dir=\"../../cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc55641",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "combined_dataset = concatenate_datasets(dataset.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cde6ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines1 = []\n",
    "lines2 = []\n",
    "for i, pair in enumerate(combined_dataset):\n",
    "    lines1.append({\"id\":str(i),\"text\":pair[\"toxic_sentence\"]})\n",
    "    lines2.append({\"id\":str(i),\"text\":pair[\"neutral_sentence\"]})\n",
    "    #print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ed20fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"toxfile.jsonl\", 'w') as f:\n",
    "    for line in lines1:\n",
    "        f.write(json.dumps(line) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b10212e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"detoxfile.jsonl\", 'w') as f:\n",
    "    for line in lines2:\n",
    "        f.write(json.dumps(line) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369a8c69",
   "metadata": {},
   "source": [
    "## EVALUATE DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429b3831",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = run_evaluation(\n",
    "        input=\"toxfile.jsonl\",\n",
    "        prediction=\"detoxfile.jsonl\",\n",
    "        output = open(\"output.json\", \"w\", encoding=\"UTF-8\"),\n",
    "        evaluator=partial(\n",
    "            evaluate_style_transfer,\n",
    "            style_model=style_model,\n",
    "            style_tokenizer=style_tokenizer,\n",
    "            meaning_model=meaning_model,\n",
    "            style_target_label=0,\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e98259c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example array of numbers\n",
    "numbers = np.sort(output['STA'])\n",
    "\n",
    "# Create a line plot\n",
    "plt.plot(numbers)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Array of Numbers')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408ac68b",
   "metadata": {},
   "source": [
    "## FILTER AND PREPARE DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae8dc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import UMT5ForConditionalGeneration, AutoTokenizer, Seq2SeqTrainer, Seq2SeqTrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4190fcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "language_prompts = {\n",
    "    \"en\": \"translate from English to English: \",\n",
    "    \"ru\": \"translate from Russian to Russian: \",\n",
    "    \"uk\": \"translate from Ukrainian to Ukrainian: \",\n",
    "    \"de\": \"translate from German to German: \",\n",
    "    \"es\": \"translate from Spanish to Spanish: \",\n",
    "    \"am\": \"translate from Amharic to Amharic: \",\n",
    "    \"zh\": \"translate from Chinese to Chinese: \",\n",
    "    \"ar\": \"translate from Arabic to Arabic: \",\n",
    "    \"hi\": \"translate from Hindi to Hindi: \",\n",
    "}\n",
    "\n",
    "# Combine datasets and add language prompts\n",
    "combined_datasets = {}\n",
    "for lang, datasett in dataset.items():\n",
    "    prompt = language_prompts[lang]\n",
    "    datasett = datasett.map(lambda example: {\"input_text\": example[\"toxic_sentence\"], \"target_text\": example[\"neutral_sentence\"]}, remove_columns=[\"toxic_sentence\", \"neutral_sentence\"])\n",
    "    combined_datasets[lang] = datasett\n",
    "\n",
    "# Concatenate all datasets\n",
    "combined_dataset = concatenate_datasets(combined_datasets.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef80f190",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(combined_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1033a363",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lang'] = 'en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92b343a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lang'][0:400] = \"en\"\n",
    "df['lang'][400:800] = \"ru\"\n",
    "df['lang'][800:1200] = \"uk\"\n",
    "df['lang'][1200:1600] = \"de\"\n",
    "df['lang'][1600:2000] = \"es\"\n",
    "df['lang'][2000:2400] = \"am\"\n",
    "df['lang'][2400:2800] = \"zh\"\n",
    "df['lang'][2800:3200] = \"ar\"\n",
    "df['lang'][3200:] = \"hi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de97ea92",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines1 = []\n",
    "lines2 = []\n",
    "for i, tox in enumerate(df[\"input_text\"]):\n",
    "    lines1.append({\"id\":str(i),\"text\":tox})\n",
    "    lines2.append({\"id\":str(i),\"text\":df[\"target_text\"][i]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181f56e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sta'] = output['STA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3497105",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df[df['sta'] > 0.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66382ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in filtered_df.index:\n",
    "    filtered_df['input_text'][i] = language_prompts[filtered_df['lang'][i]] + filtered_df['input_text'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0d6280",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493fe535",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "filtered_dataset = Dataset.from_pandas(filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2789be",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc570e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "language_prompts = {\n",
    "    \"en\": \"translate from English to English: \",\n",
    "    \"ru\": \"translate from Russian to Russian: \",\n",
    "    \"uk\": \"translate from Ukrainian to Ukrainian: \",\n",
    "    \"de\": \"translate from German to German: \",\n",
    "    \"es\": \"translate from Spanish to Spanish: \",\n",
    "    \"am\": \"translate from Amharic to Amharic: \",\n",
    "    \"zh\": \"translate from Chinese to Chinese: \",\n",
    "    \"ar\": \"translate from Arabic to Arabic: \",\n",
    "    \"hi\": \"translate from Hindi to Hindi: \",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcaa4bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasett = filtered_dataset.map(remove_columns=[\"lang\", \"sta\", \"__index_level_0__\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57f922c",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = datasett.train_test_split(test_size=0.15, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ea2a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets.save_to_disk(\"../filtered_dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636aaa43",
   "metadata": {},
   "source": [
    "## TOKENIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c19e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import UMT5ForConditionalGeneration, AutoTokenizer, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04bb02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/umt5-base\", cache_dir=\"../../cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bf609c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return  {\"input_ids\": tokenizer(examples[\"input_text\"], padding='max_length', truncation=True, max_length=512)[\"input_ids\"], \"labels\": tokenizer(examples[\"target_text\"], padding='max_length', truncation=True, max_length=128)[\"input_ids\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf1f51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = datasets.map(tokenize_function, remove_columns=[\"input_text\", \"target_text\"], num_proc=4, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96c9157",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdc3a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets.save_to_disk(\"../filtered_tokenized_datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703a23f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "executable": "/usr/bin/env python3",
   "main_language": "python",
   "notebook_metadata_filter": "-all",
   "text_representation": {
    "extension": ".py",
    "format_name": "light"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
